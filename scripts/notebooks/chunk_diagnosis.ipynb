{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes what's going on with [`long_terms_tokenizer`](https://github.com/IreneZihuiLi/HiPool/blob/main/Dataset_Split_Class.py).\n",
    "\n",
    "`long_terms_tokenizer` should really be called `chunk` or `chunk_document` since it does the following: \n",
    "\n",
    "- Tokenizes a document using BERT\n",
    "- Splits the document into chunks\n",
    "- Returns a dict with a matrix of input ids, a matrix of attention masks, etc.\n",
    "\n",
    "One of the quirks of this function is that it includes the `[CLS]` and `[SEP]` tokens in the length of the chunk, which makes calculating the overlap more complicated. In this notebook, `[CLS]` and `[SEP]` are also referred to as \"start\" and \"end\" for convenience. Here's an example that just includes the chunking logic from the function, ignoring the extra things like token type IDs, labels, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 'end']\n",
      "['start', 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 'end']\n",
      "['start', 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 'end']\n",
      "['start', 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 'end']\n",
      "['start', 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 'end']\n",
      "['start', 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 'end']\n",
      "['start', 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 'end']\n",
      "['start', 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 'end']\n",
      "['start', 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 'end']\n",
      "['start', 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 'end']\n",
      "['start', 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 'end']\n"
     ]
    }
   ],
   "source": [
    "# Toy list of tokens (all the ints from 1-100, inclusive)\n",
    "tokens = list(range(1,101))\n",
    "chunk_len = 20\n",
    "overlap_len = 10\n",
    "stride = overlap_len - 2\n",
    "number_chunks = math.floor(100/stride)\n",
    "chunks = []\n",
    "for current in range(number_chunks - 1):\n",
    "    chunk_toks = tokens[current*stride:current*stride+chunk_len-2]\n",
    "    # These are easier-to-read stand-ins for [CLS] and [SEP]\n",
    "    chunk_toks = [\"start\"] + chunk_toks + [\"end\"]  \n",
    "    chunks.append(chunk_toks)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of chunks: 12\n",
      "Actual number of chunks: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected number of chunks: {number_chunks}\")\n",
    "print(f\"Actual number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation mostly works. The overlap is correct if you don't count special tokens but this implementation causes the last chunk to be truncated because `range(number_chunks - 1)` doesn't account for the fact that the high value is exclusive.\n",
    "\n",
    "As we can see, because we are missing a chunk, we don't have full coverage.\n",
    "\n",
    "I suspect that this may have been done intentionally to avoid dealing with differently sized chunks, but that can be overcome by padding the last chunk.\n",
    "\n",
    "Below shows what happens when `number_chunks - 1` is replaced with `number_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 'end']\n",
      "['start', 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 'end']\n",
      "['start', 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 'end']\n",
      "['start', 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 'end']\n",
      "['start', 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 'end']\n",
      "['start', 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 'end']\n",
      "['start', 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 'end']\n",
      "['start', 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 'end']\n",
      "['start', 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 'end']\n",
      "['start', 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 'end']\n",
      "['start', 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 'end']\n",
      "['start', 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 'end']\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for current in range(number_chunks):\n",
    "    chunk_toks = tokens[current*stride:current*stride+chunk_len-2]\n",
    "    chunk_toks = [\"start\"] + chunk_toks + [\"end\"]\n",
    "    chunks.append(chunk_toks)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative chunking implementation\n",
    "\n",
    "This implementation does not include the start and end tokens as part of the length of the chunk. This means we can chunk:\n",
    "\n",
    "1. Chunk the document\n",
    "2. Pad the last chunk\n",
    "3. Create a tensor from the chunk.\n",
    "4. Concatenate all the start and end tokens at once.\n",
    "\n",
    "This streamlines the chunking logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
      "[31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n",
      "[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "[81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
      "[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n"
     ]
    }
   ],
   "source": [
    "# Toy list of tokens (all the ints from 1-100, inclusive)\n",
    "tokens = list(range(1,111))\n",
    "chunk_len = 20\n",
    "overlap_len = 10\n",
    "chunks = []\n",
    "current_idx = 0\n",
    "while True:\n",
    "    chunks.append(tokens[current_idx: current_idx + chunk_len])\n",
    "    if current_idx + chunk_len >= len(tokens):\n",
    "        break\n",
    "    else:\n",
    "        current_idx += chunk_len - overlap_len\n",
    "\n",
    "# Suppose -1 is pad token\n",
    "last_chunk_padding = [-1] * (chunk_len - len(chunks[-1]))\n",
    "chunks[-1] = chunks[-1] + last_chunk_padding\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create tensor and start/end tokens\n",
    "x = torch.tensor(chunks)\n",
    "# Now we'll use 101 and 102 to represent [CLS] and [SEP]\n",
    "start = torch.tensor(101).repeat(x.shape[0]).unsqueeze(dim=1)\n",
    "end = torch.tensor(102).repeat(x.shape[0]).unsqueeze(dim=1)\n",
    "y = torch.cat((start, x, end), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 22])\n",
      "tensor([[101,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20, 102],\n",
      "        [101,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,\n",
      "          24,  25,  26,  27,  28,  29,  30, 102],\n",
      "        [101,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,\n",
      "          34,  35,  36,  37,  38,  39,  40, 102],\n",
      "        [101,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
      "          44,  45,  46,  47,  48,  49,  50, 102],\n",
      "        [101,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
      "          54,  55,  56,  57,  58,  59,  60, 102],\n",
      "        [101,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
      "          64,  65,  66,  67,  68,  69,  70, 102],\n",
      "        [101,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
      "          74,  75,  76,  77,  78,  79,  80, 102],\n",
      "        [101,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90, 102],\n",
      "        [101,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
      "          94,  95,  96,  97,  98,  99, 100, 102],\n",
      "        [101,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1, 102]])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 10 chunks instead of 11 or 12 because we're including a full 20 tokens per chunk (plus 2 special tokens) instead of 18 tokens (plus 2 special tokens)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
